# æœˆçƒæœºå™¨äºº MuJoCo + DQN å¼ºåŒ–å­¦ä¹  å®Œæ•´ç§»åŠ¨è®­ç»ƒä»£ç 
# è¿è¡Œç¯å¢ƒï¼šPython3 + MuJoCo + mujoco-py + torch + gym
# ä¾èµ–å®‰è£…ï¼špip install mujoco-py gym torch numpy matplotlib
import mujoco_py
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
import matplotlib.pyplot as plt

# ---------------------- DQN ç¥ç»ç½‘ç»œï¼ˆå†³ç­–æ ¸å¿ƒï¼‰----------------------
class DQNNet(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim=64):
        super(DQNNet, self).__init__()
        # è¾“å…¥ï¼šæœˆçƒè½¦çŠ¶æ€ï¼ˆä½ç½®ã€é€Ÿåº¦ã€å§¿æ€ï¼‰ï¼Œè¾“å‡ºï¼šåŠ¨ä½œï¼ˆçº¿æ€§é€Ÿåº¦/è§’é€Ÿåº¦ï¼‰
        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

# ---------------------- æœˆçƒæœºå™¨äººå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ ----------------------
class LunarRoverDQN:
    def __init__(self, model_path="lunar_rover.xml", gamma=0.95, lr=1e-4, batch_size=32):
        # MuJoCo æœˆçƒè½¦ç‰©ç†ä»¿çœŸåˆå§‹åŒ–
        self.model = mujoco_py.load_model_from_path(model_path)
        self.sim = mujoco_py.MjSim(self.model)
        self.viewer = mujoco_py.MjViewer(self.sim)
        self.obs_dim = self.sim.data.qpos.flat.shape[0]  # çŠ¶æ€ç»´åº¦
        self.action_dim = 2  # åŠ¨ä½œç»´åº¦ï¼š[çº¿æ€§é€Ÿåº¦, è§’é€Ÿåº¦]
        self.action_bound = 0.5  # åŠ¨ä½œè¾¹ç•Œï¼ˆæœˆçƒä½é‡åŠ›ï¼Œé€Ÿåº¦ä¸å®œè¿‡å¤§ï¼‰

        # DQN æ ¸å¿ƒå‚æ•°
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.lr = lr        # å­¦ä¹ ç‡
        self.batch_size = batch_size
        self.memory = deque(maxlen=10000)  # ç»éªŒå›æ”¾æ± 

        # æ„å»ºDQNç½‘ç»œï¼ˆè¯„ä¼°ç½‘ç»œ + ç›®æ ‡ç½‘ç»œï¼‰
        self.eval_net = DQNNet(self.obs_dim, self.action_dim)
        self.target_net = DQNNet(self.obs_dim, self.action_dim)
        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=self.lr)
        self.loss_func = nn.MSELoss()

        # åŒæ­¥ç›®æ ‡ç½‘ç»œå‚æ•°
        self.target_net.load_state_dict(self.eval_net.state_dict())

    # åŠ¨ä½œé€‰æ‹©ï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼šæ¢ç´¢+åˆ©ç”¨ï¼‰
    def choose_action(self, obs, epsilon=0.1):
        obs = torch.FloatTensor(obs).unsqueeze(0)
        if random.random() > epsilon:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            action = self.eval_net(obs).detach().numpy()[0]
        else:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            action = np.random.uniform(-self.action_bound, self.action_bound, self.action_dim)
        return action

    # å­˜å‚¨ç»éªŒï¼ˆs, a, r, s'ï¼‰
    def store_experience(self, s, a, r, s_, done):
        self.memory.append((s, a, r, s_, done))

    # ç»éªŒå›æ”¾è®­ç»ƒ
    def learn(self):
        if len(self.memory) < self.batch_size:
            return  # ç»éªŒæ± ä¸è¶³ï¼Œä¸è®­ç»ƒ

        # æŠ½å–æ‰¹é‡ç»éªŒ
        batch = random.sample(self.memory, self.batch_size)
        s_batch = torch.FloatTensor([x[0] for x in batch])
        a_batch = torch.FloatTensor([x[1] for x in batch])
        r_batch = torch.FloatTensor([x[2] for x in batch]).unsqueeze(1)
        s_next_batch = torch.FloatTensor([x[3] for x in batch])
        done_batch = torch.FloatTensor([x[4] for x in batch]).unsqueeze(1)

        # è®¡ç®—è¯„ä¼°ç½‘ç»œQå€¼
        q_eval = self.eval_net(s_batch)
        # è®¡ç®—ç›®æ ‡ç½‘ç»œQå€¼ï¼ˆBellmanæ–¹ç¨‹ï¼‰
        q_target = r_batch + self.gamma * self.target_net(s_next_batch).detach() * (1 - done_batch)

        # è®¡ç®—æŸå¤±ï¼Œåå‘ä¼ æ’­
        loss = self.loss_func(q_eval, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    # é‡ç½®æœºå™¨äººçŠ¶æ€ï¼ˆæœˆçƒè½¦å›åˆ°åˆå§‹ä½ç½®ï¼‰
    def reset(self):
        self.sim.reset()
        return self.sim.data.qpos.flat[:]

    # æ‰§è¡Œä¸€æ­¥åŠ¨ä½œï¼Œè®¡ç®—å¥–åŠ±ï¼ˆæ ¸å¿ƒï¼šé€‚é…æœˆçƒç§»åŠ¨ï¼‰
    def step(self, action):
        self.sim.data.ctrl[:] = action  # æ‰§è¡ŒåŠ¨ä½œ
        self.sim.step()
        s_ = self.sim.data.qpos.flat[:]  # ä¸‹ä¸€ä¸ªçŠ¶æ€

        # ---------------------- æœˆçƒä¸“å±å¥–åŠ±å‡½æ•° ----------------------
        # 1. è·ç¦»å¥–åŠ±ï¼šé è¿‘ç›®æ ‡ç‚¹ï¼ˆå‡è®¾ç›®æ ‡ç‚¹åœ¨(0,0)ï¼‰
        distance = np.linalg.norm(s_[:2])
        dist_reward = -0.1 * distance
        # 2. é€Ÿåº¦å¥–åŠ±ï¼šä¿æŒç¨³å®šé€Ÿåº¦ï¼ˆæœˆçƒä½é‡åŠ›ï¼Œé¿å…é€Ÿåº¦è¿‡å¤§/è¿‡å°ï¼‰
        speed = np.linalg.norm(action[0])
        speed_reward = 0.2 if 0.1 <= speed <= 0.3 else -0.1
        # 3. å§¿æ€å¥–åŠ±ï¼šé¿å…æœˆçƒè½¦å€¾å€’ï¼ˆå§¿æ€è§’æ¥è¿‘0åˆ™å¥–åŠ±ï¼‰
        pose_reward = -0.05 * np.abs(s_[2])
        # æ€»å¥–åŠ±
        reward = dist_reward + speed_reward + pose_reward

        # ç»ˆæ­¢æ¡ä»¶ï¼šåˆ°è¾¾ç›®æ ‡ç‚¹ æˆ– å€¾å€’ æˆ– è¶…å‡ºè¾¹ç•Œ
        done = False
        if distance < 0.1:  # åˆ°è¾¾ç›®æ ‡ç‚¹
            reward += 10  # é¢å¤–å¥–åŠ±
            done = True
        if np.abs(s_[2]) > np.pi/4:  # å€¾å€’
            reward -= 5   # æƒ©ç½š
            done = True
        if distance > 5:  # è¶…å‡ºè¾¹ç•Œ
            done = True

        return s_, reward, done

    # å®Œæ•´è®­ç»ƒ+å¯è§†åŒ–è¿è¡Œ
    def train(self, episodes=50, epsilon_decay=0.995):
        loss_list = []
        reward_list = []
        epsilon = 0.3  # åˆå§‹æ¢ç´¢ç‡

        for ep in range(episodes):
            s = self.reset()
            done = False
            total_reward = 0
            episode_loss = 0
            step_cnt = 0

            while not done:
                self.viewer.render()  # å¯è§†åŒ–æœˆçƒè½¦ç§»åŠ¨
                step_cnt += 1

                # é€‰æ‹©åŠ¨ä½œï¼Œæ‰§è¡ŒåŠ¨ä½œ
                action = self.choose_action(s, epsilon)
                s_, reward, done = self.step(action)

                # å­˜å‚¨ç»éªŒï¼Œå­¦ä¹ 
                self.store_experience(s, action, reward, s_, done)
                loss = self.learn()
                if loss:
                    episode_loss += loss

                # æ›´æ–°çŠ¶æ€å’Œæ€»å¥–åŠ±
                s = s_
                total_reward += reward

            # æ¢ç´¢ç‡è¡°å‡ï¼ˆè¶Šè®­ç»ƒï¼Œè¶Šå€¾å‘äºåˆ©ç”¨ï¼‰
            epsilon *= epsilon_decay
            # æ¯10ä¸ªepisodeåŒæ­¥ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
            if (ep + 1) % 10 == 0:
                self.target_net.load_state_dict(self.eval_net.state_dict())

            # è®°å½•æ•°æ®
            loss_list.append(episode_loss / step_cnt if step_cnt !=0 else 0)
            reward_list.append(total_reward)
            print(f"ğŸ“Œ Episode {ep+1:2d} | Total Reward: {total_reward:.2f} | Avg Loss: {loss_list[-1]:.4f} | Epsilon: {epsilon:.3f}")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_train_curve(reward_list, loss_list)
        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        torch.save(self.eval_net.state_dict(), "lunar_rover_dqn.pth")
        print("\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸ºï¼šlunar_rover_dqn.pth")

    # ç»˜åˆ¶è®­ç»ƒå¥–åŠ±/æŸå¤±æ›²çº¿
    def plot_train_curve(self, rewards, losses):
        plt.figure(figsize=(12, 4))
        plt.subplot(1,2,1)
        plt.plot(rewards, label="Total Reward")
        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.legend()
        plt.title("Lunar Rover DQN Training Reward")

        plt.subplot(1,2,2)
        plt.plot(losses, label="Average Loss", color="orange")
        plt.xlabel("Episode")
        plt.ylabel("Loss")
        plt.legend()
        plt.title("Lunar Rover DQN Training Loss")
        plt.show()

# ---------------------- æµ‹è¯•ä»£ç ï¼ˆç›´æ¥è¿è¡Œï¼‰----------------------
if __name__ == "__main__":
    # æ³¨æ„ï¼šæ›¿æ¢ä¸ºä½ çš„æœˆçƒè½¦MuJoCoæ¨¡å‹è·¯å¾„ï¼ˆxmlæ–‡ä»¶ï¼‰
    rover_agent = LunarRoverDQN(model_path="lunar_rover.xml")
    # å¯åŠ¨è®­ç»ƒï¼ˆ50ä¸ªepisodeï¼Œå¯æ ¹æ®ç”µè„‘æ€§èƒ½è°ƒæ•´ï¼‰
    rover_agent.train(episodes=50)
